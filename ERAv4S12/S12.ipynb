{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PBCp9UGNCDp",
    "outputId": "4e860907-1af4-401b-a77d-2704bf96c8e4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "step     0 | loss: 10.972174 | lr: 6.00e-07\n",
      "step    50 | loss: 8.288091 | lr: 3.06e-05\n",
      "step   100 | loss: 6.635241 | lr: 6.06e-05\n",
      "step   150 | loss: 5.824837 | lr: 9.06e-05\n",
      "step   200 | loss: 5.482788 | lr: 1.21e-04\n",
      "step   250 | loss: 5.265896 | lr: 1.51e-04\n",
      "step   300 | loss: 5.158043 | lr: 1.81e-04\n",
      "step   350 | loss: 4.483272 | lr: 2.11e-04\n",
      "step   400 | loss: 3.955303 | lr: 2.41e-04\n",
      "step   450 | loss: 4.284541 | lr: 2.71e-04\n",
      "step   500 | loss: 3.639950 | lr: 3.01e-04\n",
      "step   550 | loss: 4.191215 | lr: 3.31e-04\n",
      "step   600 | loss: 3.790927 | lr: 3.61e-04\n",
      "step   650 | loss: 3.433627 | lr: 3.91e-04\n",
      "step   700 | loss: 3.211645 | lr: 4.21e-04\n",
      "step   750 | loss: 3.100039 | lr: 4.51e-04\n",
      "step   800 | loss: 3.396965 | lr: 4.81e-04\n",
      "step   850 | loss: 3.200209 | lr: 5.11e-04\n",
      "step   900 | loss: 2.642825 | lr: 5.41e-04\n",
      "step   950 | loss: 2.456589 | lr: 5.71e-04\n",
      "step  1000 | loss: 2.557349 | lr: 6.00e-04\n",
      "step  1050 | loss: 2.634692 | lr: 6.00e-04\n",
      "step  1100 | loss: 2.201802 | lr: 6.00e-04\n",
      "step  1150 | loss: 1.623584 | lr: 6.00e-04\n",
      "step  1200 | loss: 1.633972 | lr: 6.00e-04\n",
      "step  1250 | loss: 1.495467 | lr: 6.00e-04\n",
      "step  1300 | loss: 1.514711 | lr: 6.00e-04\n",
      "step  1350 | loss: 1.043494 | lr: 6.00e-04\n",
      "step  1400 | loss: 1.172354 | lr: 6.00e-04\n",
      "step  1450 | loss: 0.796930 | lr: 6.00e-04\n",
      "step  1500 | loss: 0.852021 | lr: 6.00e-04\n",
      "step  1550 | loss: 0.597007 | lr: 6.00e-04\n",
      "step  1600 | loss: 0.590056 | lr: 6.00e-04\n",
      "step  1650 | loss: 0.632478 | lr: 6.00e-04\n",
      "step  1700 | loss: 0.326799 | lr: 6.00e-04\n",
      "step  1750 | loss: 0.365723 | lr: 6.00e-04\n",
      "step  1800 | loss: 0.310584 | lr: 6.00e-04\n",
      "step  1850 | loss: 0.296524 | lr: 6.00e-04\n",
      "step  1900 | loss: 0.294781 | lr: 6.00e-04\n",
      "step  1950 | loss: 0.290859 | lr: 6.00e-04\n",
      "step  2000 | loss: 0.241131 | lr: 6.00e-04\n",
      "step  2050 | loss: 0.252659 | lr: 6.00e-04\n",
      "step  2100 | loss: 0.222209 | lr: 6.00e-04\n",
      "step  2150 | loss: 0.205375 | lr: 6.00e-04\n",
      "step  2200 | loss: 0.242598 | lr: 6.00e-04\n",
      "step  2250 | loss: 0.199048 | lr: 6.00e-04\n",
      "step  2300 | loss: 0.200101 | lr: 6.00e-04\n",
      "step  2350 | loss: 0.207331 | lr: 6.00e-04\n",
      "step  2400 | loss: 0.185837 | lr: 6.00e-04\n",
      "step  2450 | loss: 0.171859 | lr: 6.00e-04\n",
      "step  2500 | loss: 0.177857 | lr: 6.00e-04\n",
      "step  2550 | loss: 0.143138 | lr: 6.00e-04\n",
      "step  2600 | loss: 0.167301 | lr: 6.00e-04\n",
      "step  2650 | loss: 0.173609 | lr: 6.00e-04\n",
      "step  2700 | loss: 0.173656 | lr: 6.00e-04\n",
      "step  2750 | loss: 0.155061 | lr: 6.00e-04\n",
      "step  2800 | loss: 0.179392 | lr: 6.00e-04\n",
      "step  2850 | loss: 0.127104 | lr: 6.00e-04\n",
      "step  2900 | loss: 0.148127 | lr: 6.00e-04\n",
      "step  2950 | loss: 0.152990 | lr: 6.00e-04\n",
      "step  3000 | loss: 0.146374 | lr: 6.00e-04\n",
      "step  3050 | loss: 0.138948 | lr: 6.00e-04\n",
      "step  3100 | loss: 0.137794 | lr: 6.00e-04\n",
      "step  3150 | loss: 0.155247 | lr: 6.00e-04\n",
      "step  3200 | loss: 0.130345 | lr: 6.00e-04\n",
      "step  3250 | loss: 0.143885 | lr: 6.00e-04\n",
      "step  3300 | loss: 0.146641 | lr: 6.00e-04\n",
      "step  3350 | loss: 0.116431 | lr: 6.00e-04\n",
      "step  3400 | loss: 0.145148 | lr: 6.00e-04\n",
      "step  3450 | loss: 0.109510 | lr: 6.00e-04\n",
      "step  3500 | loss: 0.120601 | lr: 6.00e-04\n",
      "step  3550 | loss: 0.117872 | lr: 6.00e-04\n",
      "step  3600 | loss: 0.120572 | lr: 6.00e-04\n",
      "step  3650 | loss: 0.108940 | lr: 6.00e-04\n",
      "step  3700 | loss: 0.114175 | lr: 6.00e-04\n",
      "step  3750 | loss: 0.118673 | lr: 6.00e-04\n",
      "step  3800 | loss: 0.108108 | lr: 6.00e-04\n",
      "step  3850 | loss: 0.137382 | lr: 6.00e-04\n",
      "step  3900 | loss: 0.129900 | lr: 6.00e-04\n",
      "step  3950 | loss: 0.107007 | lr: 6.00e-04\n",
      "step  4000 | loss: 0.115533 | lr: 6.00e-04\n",
      "step  4050 | loss: 0.116073 | lr: 6.00e-04\n",
      "step  4100 | loss: 0.105176 | lr: 6.00e-04\n",
      "\n",
      "Target achieved at step 4109!\n",
      "Final loss: 0.08488673716783524\n"
     ]
    }
   ],
   "source": [
    "# Solving for residual std scaling issue\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "B = 16\n",
    "T = 64\n",
    "train_loader = DataLoaderLite(B=B, T=T)\n",
    "\n",
    "# Training hyperparameters\n",
    "max_steps = 300000\n",
    "grad_accum_steps = 4\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 1000\n",
    "grad_clip = 1.0\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=max_lr,betas=(0.91, 0.95),weight_decay=0.1)\n",
    "model.train()\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    elif device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (B * T * grad_accum_steps) / (t1 - t0)\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f'step {step:5d} | loss: {loss_accum.item():.6f} | lr: {lr:.2e}')\n",
    "\n",
    "    # Check if target achieved\n",
    "    if loss_accum.item() < 0.09:\n",
    "        print(f'\\nTarget achieved at step {step}!')\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'Final loss: {loss_accum}')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "while x.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  }
 ]
}
