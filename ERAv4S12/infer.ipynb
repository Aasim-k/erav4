{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PBCp9UGNCDp",
        "outputId": "91686185-8ee0-4f9e-c92d-e91b3b39d60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loaded checkpoint with model_state_dict\n",
            "Training loss: 0.08488673716783524\n",
            "\n",
            "----- SAMPLE OUTPUT 1 -----\n",
            "Once upon a time calm.\n",
            "That Henry yields thus said,\n",
            "And thus ne'er burn as light in London send his hands,\n",
            "Were buckle beautled only dislike:\n",
            "With many misforce's course, calm enter, with a titportion'd thy father!\n",
            "\n",
            "ROMEO:\n",
            "Good time, mad!\n",
            "\n",
            "ROMEO:\n",
            "I will not believe a buried man! yet, nurseful brother's house, as Oumerle, madars, madre we be bold in such woe\n",
            "That sorrow that his high disgrace and measure of philosophy.\n",
            "\n",
            "ROMEO:\n",
            "What says I am told to fagns best Paris.\n",
            "An old blocks's groarth, hath Romeo's short sudden with Saint George-day's\n",
            "\n",
            "----- SAMPLE OUTPUT 2 -----\n",
            "The meaning of life is accounted pay Lancaster with red bones\n",
            "person straight; at Lancaster; where they have?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Go on that we blow strong; to bear me leave.\n",
            "\n",
            "HENRY PERCY:\n",
            "My lord, my Lord with English choice, Cbot!'\n",
            "ever till me to rise, that we love in vain;\n",
            "The town since this vile strife, as it have ta'encas andHarry,\n",
            "Poor queen is true as many free Troy?\n",
            "\n",
            "HENRY PERCY:\n",
            "Go, who areever till what true haplylling much sound loose presence 'tis so good friend, when any time as much alone. Go through the lateste' high\n",
            "\n",
            "----- SAMPLE OUTPUT 3 -----\n",
            "In the future, in banisp on his throne,\n",
            "Our householdcon staints on his bosom,\n",
            "As ashes generally an unsturhes to his rest.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Have denied the while of Oxford, and be ill-bret shame,\n",
            "Before not my breath; no less curt lies to temper tears.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Yet that I do yet to get us home,\n",
            "Before thy to triumph that I am in peacety alone:\n",
            "Thy Romeo! then call him hard salt deed.\n",
            "\n",
            "Hence o'er flamed stain him; I do spend my word, I do that ere you so will accuse my wife, I had as\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from model import GPT, GPTConfig\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def load_model(checkpoint_path=\"model.pt\"):\n",
        "    config = GPTConfig()\n",
        "    model = GPT(config)\n",
        "\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"{checkpoint_path} not found\")\n",
        "\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Case A: full model saved\n",
        "    if isinstance(ckpt, GPT):\n",
        "        print(\"Loaded full model object\")\n",
        "        model = ckpt\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    # Case B: wrapped checkpoint {\"model_state_dict\": ...}\n",
        "    if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
        "        print(\"Loaded checkpoint with model_state_dict\")\n",
        "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        print(\"Training loss:\", ckpt.get(\"loss\", \"N/A\"))\n",
        "    else:\n",
        "        # Case C: raw state dict\n",
        "        print(\"Loaded raw state_dict\")\n",
        "        model.load_state_dict(ckpt)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Sampling function\n",
        "# ----------------------------------------------------------------------------\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def generate(model, prompt, max_new_tokens=100, temperature=1.0, top_k=50):\n",
        "    model.eval()\n",
        "    # Encode prompt\n",
        "    tokens = enc.encode(prompt)\n",
        "    idx = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Forward pass\n",
        "        logits, _ = model(idx)\n",
        "\n",
        "        # Take last token logits\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Top-k filtering\n",
        "        if top_k is not None:\n",
        "            values, indices = torch.topk(logits, k=top_k)\n",
        "            logits = logits.masked_fill(logits < values[:, -1:], float('-inf'))\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sample token\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append to sequence\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "    return enc.decode(idx[0].tolist())\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Run sample generations\n",
        "# ----------------------------------------------------------------------------\n",
        "model = load_model(\"model.pt\")\n",
        "\n",
        "print(\"\\n----- SAMPLE OUTPUT 1 -----\")\n",
        "print(generate(model, \"Once upon a time\", max_new_tokens=150))\n",
        "\n",
        "print(\"\\n----- SAMPLE OUTPUT 2 -----\")\n",
        "print(generate(model, \"The meaning of life is\", max_new_tokens=150))\n",
        "\n",
        "print(\"\\n----- SAMPLE OUTPUT 3 -----\")\n",
        "print(generate(model, \"In the future,\", max_new_tokens=150))"
      ]
    }
  ]
}